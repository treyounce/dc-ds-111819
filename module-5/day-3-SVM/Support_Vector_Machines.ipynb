{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "__objective__:\n",
    "    -  Maximal Margin Classifier\n",
    "    -  Soft Margin Classifier\n",
    "    -  Example\n",
    "    -  Kernels\n",
    "  \n",
    "__Note__: This notebook is adapted from ch:9 of ISLR. \n",
    "  \n",
    "__Classification Algorithms__\n",
    "\n",
    " - Logistic Regression\n",
    " - Knn\n",
    " - Decision Trees \n",
    " - Random Forests\n",
    "  \n",
    "## First a little bit prerequisite math\n",
    "\n",
    "[line equation with desmos](https://www.desmos.com/calculator/tzz7jlc6vr)\n",
    "\n",
    "[Plane equation with GeoGebra](https://www.geogebra.org/3d/pxwfdmdx)\n",
    "\n",
    "\n",
    "\n",
    "## Maximal Margin Classifier\n",
    "\n",
    "In classification one natural idea might be to try to find a hyperplane (line, plane, etc) that perfectly separates the classess.\n",
    "\n",
    "<img src=\"img/maximal_margin_classifier.png\" width=450, height=450> \n",
    "\n",
    "\n",
    "- Let's say we are lucky enough to find such plane then the question becomes among infinitely many planes that separates the classes which one is the 'best' choice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Maximal Margin Classifier__ addresses this problem by choosing the plane that maximizes the margin (furthest from the training observations).\n",
    "\n",
    "<img src=\"img/support_vectors.png\" width=450, height=450> \n",
    " \n",
    "__Q__: We have a little problem though! How can we find this, in other words, what is the problem that we would like to solve?\n",
    "\n",
    "In fact, it is a very easy one:\n",
    "\n",
    "<img src=\"img/constructing_maximal_margin.png\" width=450, height=450> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we can also write this problem as:\n",
    "\n",
    "<img src=\"img/svc_problem.png\" width=350, height=350> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into this problem, do you see why maximal margin classifier might be a problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 1 supplement.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Support Vector Classifier (soft margin classifier)\n",
    "\n",
    "\n",
    ">  In fact, even if a separating hyper-plane does exist, then there are instances in which a classifier based on a separating hyperplane might not be desirable. A classifier based on a separating hyperplane will necessarily perfectly classify all of the training observations; this can lead to sensitivity to individual observations.\n",
    "\n",
    "We will sacrifice the perfect separation for the sake of \n",
    "\n",
    " - Robustness\n",
    " - Better test performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q:__ How are we going to write this problem mathematically?\n",
    "<img src=\"img/slack_variables.png\" width=450, height=450> \n",
    "\n",
    "- $\\xi_{i}$ are called slack variables and this problem is equivalent to: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/svc_slack.png\" width=450, height=450> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q__: Let's investigate the role of $\\xi_{i}$:\n",
    "\n",
    "- What happens if $\\xi_{i} = 0$?\n",
    "\n",
    "- What if $0<\\xi_{i} < 1$?\n",
    "\n",
    "- $\\xi_{i} >1$\n",
    "\n",
    "\n",
    "__Q__: How about the role of C?\n",
    "\n",
    "- if C = 0 then what are the possible values for $\\epsilon_{i}$?\n",
    "\n",
    "- How many misclassification would be allowed if C > 0  and $\\epsilon_{i} >1$?\n",
    "\n",
    "\n",
    "__Note__: C is a hyper-parameter of the model so the best value of it determined by cross-validation. \n",
    "\n",
    "<img src=\"img/effect_of_C.png\" width=450, height=450> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that once this problem is solved than we will make predictions by the sign of: \n",
    "\n",
    "$$ f(x^{\\ast}) = \\beta_{0} + \\beta_{1}x^{\\ast}_{1} + \\cdots + \\beta_{p}x^{\\ast}_{p}$$\n",
    "\n",
    "\n",
    "__Q:__ Support Vector Classifiers a lot better than maximal margin classifiers but still not enough for most of the applications. Can you see why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load -r 2 supplement.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data from [this site](http://archive.ics.uci.edu/ml/datasets/banknote+authentication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata = pd.read_csv('data/data_banknote_authentication.txt', header=None)\n",
    "bankdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our data doesn't have header, so we will manually add that on \n",
    "headers = [\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\", \"Class\"]\n",
    "bankdata.columns = headers\n",
    "\n",
    "bankdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_train, return_counts= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "\n",
    "sns.pairplot(bankdata[[\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\", 'Class']],\n",
    "             hue = 'Class');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Train-Test-Split__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bankdata.drop('Class', axis=1)  \n",
    "y = bankdata['Class'] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 1029) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Scaling Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Turn__ \n",
    "\n",
    "- Train a support vector classifier with 5 fold cross validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 4-15 supplement.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv['test_score'])\n",
    "estimator = cv['estimator'][1]\n",
    "\n",
    "y_pred = estimator.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix , accuracy_score\n",
    "print(classification_report(y_train, y_pred)) \n",
    "print(f\"The accuracy score is {accuracy_score(y_train, y_pred)}\")\n",
    "\n",
    "\n",
    "conf_matrix = pd.DataFrame(confusion_matrix(y_train, y_pred),\n",
    "                           index = ['actual 0', 'actual 1'], \n",
    "                           columns = ['predicted 0', 'predicted 1'])\n",
    "conf_matrix\n",
    "\n",
    "\n",
    "## https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's check the documentation\n",
    "## https://scikit-learn.org/stable/modules/svm.html#classification\n",
    "## https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! Wait a minute: The title of the lecture is Support vector machines but we didn't see them yet?\n",
    "\n",
    "\n",
    "### Support Vector Machines\n",
    "\n",
    "- Very basic idea: Transform the inputs into another space!!\n",
    "\n",
    " - Add higher orders (More Features)\n",
    " - Find smart transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Resources:\n",
    "\n",
    "[Cornell - Machine Learning](https://www.youtube.com/watch?v=FgTQG2IozlM)\n",
    "\n",
    "[Andrew Ng - SVM - RBF kernel-1](https://www.youtube.com/watch?v=mTyT-oHoivA)\n",
    "\n",
    "[Andrew Ng - SVM - Kernels and practical advices-III](https://www.youtube.com/watch?v=FCUBwP-JTsA)\n",
    "\n",
    "Also, read ch: 9.3 from ISLR. If you want more technical explanation read 12.2 and 12.3 from Elements of Statistical Learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
